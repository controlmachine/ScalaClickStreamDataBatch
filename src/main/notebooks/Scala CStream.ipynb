{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project : ClickStream Data Analysis Using YooChoose Competition Data\n",
    "## Data Scientist : Kemal Emre Ã‡olak\n",
    "### Problem Derivation-SubProblem = This problem was a question that getting asked by YooChoose Competition Community. Unfortunately, there is no latest summission file that I can check my solution. \n",
    "\n",
    "#### Problem: Predict that user will buy or not in this session with given ClickStream Data.\n",
    "\n",
    "Note that this is analysis phase of full project. Project is implemented using Scala-Frameless(Spark) by using a lot of patterns and use cases. Reason behind I use Frameless is Type-Safetiness of DataFrames-DataSets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-OF67SJU:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1602135701279)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j.{Level, Logger}\r\n",
       "import org.apache.spark.SparkConf\r\n",
       "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\r\n",
       "import org.apache.spark.sql.functions._\r\n",
       "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, MulticlassClassificationEvaluator}\r\n",
       "import org.apache.spark.ml.feature.{StringIndexer, StandardScaler, MinMaxScaler, MaxAbsScaler}\r\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, BinaryLogisticRegressionSummary, RandomForestClassifier, DecisionTreeClassifier, GBTClassifier}\r\n",
       "import org.apache.spark.ml.param.ParamMap\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "import org.apache.spark.sql.functions.rand\r\n",
       "import org.apache.spark.ml.{Pipeline, PipelineStage}\r\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGri..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Defining imports\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator,MulticlassClassificationEvaluator}\n",
    "import org.apache.spark.ml.feature.{StringIndexer,StandardScaler,MinMaxScaler,MaxAbsScaler}\n",
    "import org.apache.spark.ml.classification.{LogisticRegression,BinaryLogisticRegressionSummary,RandomForestClassifier,DecisionTreeClassifier,GBTClassifier}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions.rand\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.expressions.UserDefinedFunction\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,ChiSqSelector}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cStreamDF: org.apache.spark.sql.DataFrame = [sessID: string, date: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Loading ClickStreamData and transform it case class(sessID:string,date:string,itemID:string,type:string) format.\n",
    "val cStreamDF = spark.read.format(\"csv\")\n",
    "    .option(\"header\",\"TRUE\")\n",
    "    .option(\"delimeter\",\",\")\n",
    "    .load(\"yoochoose-clicks.dat\")\n",
    "    .toDF(\"sessID\",\"date\",\"itemID\",\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cStreamBasketDF: org.apache.spark.sql.DataFrame = [sessID: string, date: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Loading ClickStreamSession Buy Data and transform it case class(sessID:string,date:string,itemID:string,price:int,qty:string) format.\n",
    "val cStreamBasketDF = spark.read.format(\"csv\")\n",
    "    .option(\"header\",\"TRUE\")\n",
    "    .option(\"delimeter\",\",\")\n",
    "    .load(\"yoochoose-buys.dat\")\n",
    "    .toDF(\"sessID\",\"date\",\"itemID\",\"price\",\"qty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dateInfExtractor: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Date, hour, day informations are extracted once before using spark  regex and date functions.\n",
    "def dateInfExtractor(df: DataFrame): DataFrame = df.withColumn(\"date\", regexp_replace(regexp_extract(col(\"date\"),\n",
    "\"[0-9]{4}-[0-9]{2}-[0-9]{2}[A-Za-z][0-9]{2}:[0-9]{2}:[0-9]{2}.[0-9]{3}\", 0), \"[A-Za-z]\",\" \"))\n",
    "                                            .withColumn(\"sDate\", regexp_extract(col(\"date\"),\"[0-9]{4}-[0-9]{2}-[0-9]{2}\",0))\n",
    "                                            .withColumn(\"day\",date_format(col(\"date\"),\"E\"))\n",
    "                                            .withColumn(\"hour\",hour(col(\"date\")))\n",
    "                                            .withColumn(\"hStatus\"\n",
    "                                            ,when(hour(col(\"date\"))>=2 && hour(col(\"date\"))<6, \"latenight\")\n",
    "                                            .when(hour(col(\"date\"))>=6 && hour(col(\"date\"))<12, \"morning\")\n",
    "                                            .when(hour(col(\"date\"))>=12 && hour(col(\"date\"))<18, \"afternoon\")\n",
    "                                            .when(hour(col(\"date\"))>=18 && hour(col(\"date\"))<=22, \"evening\")\n",
    "                                            .otherwise(\"midnight\")\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weekTypeExtractor: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Because of buying behaviour of user, I tried to check days in order to create baseline model.\n",
    "def weekTypeExtractor(df : DataFrame): DataFrame = df.withColumn(\"weekend\",\n",
    "                                                                when(col(\"day\")===\"Sun\" || col(\"day\")===\"Sat\",1)\n",
    "                                                               otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "specOfferExtractor: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Looking for is there any relationship between another subcategory of type Column.\n",
    "def specOfferExtractor(df:DataFrame) : DataFrame  = df.withColumn(\"specOffer\"\n",
    "                                                                ,when(col(\"type\")===\"S\", \"special\")\n",
    "                                                                .when(col(\"type\")===0, \"missing\")\n",
    "                                                                .when(length(col(\"type\"))>=8, \"brand\")\n",
    "                                                                .otherwise(\"normal\")\n",
    "                                                               )            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sessAverageItemPriceViewed: (dfStream: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sessAverageItemPriceViewed(dfStream: DataFrame) ={\n",
    "    val cStreamBasketDFBeta = cStreamBasketDF\n",
    "    .select(\"itemID\",\"price\")\n",
    "    .dropDuplicates(\"itemID\")\n",
    "    .withColumnRenamed(\"itemID\",\"itemID1\")\n",
    "    \n",
    "    val updatedDFStream = dfStream.join(cStreamBasketDFBeta,dfStream(\"itemID\")===cStreamBasketDFBeta(\"itemID1\"),\"left\").drop(\"itemID1\")\n",
    "    .withColumn(\"price\",col(\"price\").cast(DoubleType)).groupBy(\"sessID\").agg(sum(\"price\"),count(\"sessID\"),avg(\"price\"),stddev(\"price\"))\n",
    "    .withColumnRenamed(\"sessID\",\"sessID1\").withColumnRenamed(\"count(sessID)\",\"nOfPVis\")\n",
    "    \n",
    "    dfStream.join(updatedDFStream,dfStream(\"sessID\")===updatedDFStream(\"sessID1\"),\"inner\").drop(\"sessID1\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sessAverageTimeBSessions: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sessAverageTimeBSessions(df: DataFrame)  ={\n",
    "    val names = Window.partitionBy('sessID).orderBy('date.asc)\n",
    "\n",
    "    val finalPrDF = df\n",
    "      .withColumn(\"nexttime\", lead(col(\"date\"), 1) over names)\n",
    "      .withColumn(\"timeBSess\", datediff(col(\"nexttime\"),\n",
    "        col(\"date\")).cast(IntegerType)).groupBy(\"sessID\").agg(avg(\"timeBSess\"))\n",
    "       .withColumnRenamed(\"sessID\",\"sessID1\")\n",
    "    \n",
    "  df.join(finalPrDF,df(\"sessID\")===finalPrDF(\"sessID1\"),\"inner\").drop(\"sessID1\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timeSpentEachSession: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// How much user spent time in each session?\n",
    "def timeSpentEachSession(df: DataFrame) = df.join(df.groupBy(\"sessID\").agg(min(\"date\"),max(\"date\"))\n",
    "                                .withColumn(\"passedInSess(minute)\", (to_timestamp(col(\"max(date)\")).cast(LongType)-\n",
    "                                                                           to_timestamp(col(\"min(date)\")).cast(LongType))/60)\n",
    "                                .drop(\"min(date)\",\"max(date)\"),Seq(\"sessID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "itemDayPopularityExtractor: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// How many click gets the item in this day?\n",
    "def itemDayPopularityExtractor(df:DataFrame) = {\n",
    "    val df1 = df.groupBy(\"sDate\",\"itemID\").count()\n",
    "            .withColumnRenamed(\"sDate\",\"sDate1\")\n",
    "            .withColumnRenamed(\"itemID\",\"itemID1\")\n",
    "    df.join(df1,df(\"sDate\")===df1(\"sDate1\") && df(\"itemID\")===df1(\"itemID1\")).drop(\"sDate1\",\"itemID1\")\n",
    "    .withColumnRenamed(\"count\",\"clicksInDay\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "itemGeneralPopularityExtractor: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// How many click gets the item for all times?\n",
    "def itemGeneralPopularityExtractor(df:DataFrame) = {\n",
    "    val df1 = df.groupBy(\"itemID\").count()\n",
    "            .withColumnRenamed(\"itemID\",\"itemID1\")\n",
    "    df.join(df1,df(\"itemID\")===df1(\"itemID1\")).drop(\"itemID1\").withColumnRenamed(\"count\",\"clicksInAll\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dayPopularityExtractor: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// How popular is day?\n",
    "def dayPopularityExtractor(df:DataFrame) = {\n",
    "    val df1 = df\n",
    "    .groupBy(\"sDate\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"sDate\",\"sDate1\")\n",
    "    .withColumnRenamed(\"count\",\"dayPopularity\")\n",
    "    df.join(df1,df(\"sDate\")===df1(\"sDate1\")).drop(\"sDate1\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userPathExtractor: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def userPathExtractor(df:DataFrame) = df.select(\"sessID\",\"itemID\",\"fullDate\",\"type\")\n",
    "    .withColumn(\"userPath\",collect_set(\"type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dateExtractCombinerCStreamBase: org.apache.spark.sql.DataFrame => org.apache.spark.sql.DataFrame = <function1>\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Functional programming concepts to function composition\n",
    "val dateExtractCombinerCStreamBase = dateInfExtractor _ andThen \n",
    "                          weekTypeExtractor _ andThen \n",
    "                          specOfferExtractor _ andThen \n",
    "                          timeSpentEachSession _ andThen\n",
    "                          dayPopularityExtractor _ andThen\n",
    "                          itemDayPopularityExtractor _ andThen\n",
    "                          itemGeneralPopularityExtractor _ \n",
    "// Since buy data for this problem only will be used for labelling, there is no need different extractor for Purchase Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dateInformationsExtractedDF: org.apache.spark.sql.DataFrame = [sessID: string, date: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dateInformationsExtractedDF = dateExtractCombinerCStreamBase(cStreamDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "messyPreDF: org.apache.spark.sql.DataFrame = [sessID: string, type: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val messyPreDF = dateInformationsExtractedDF\n",
    "    .join(cStreamBasketDF.groupBy(\"sessID\").count(),Seq(\"sessID\"),\"left\")\n",
    "    .withColumnRenamed(\"count\",\"purchased\")\n",
    "    .withColumn(\"purchased\"\n",
    "    ,when(col(\"purchased\")>0,1)\n",
    "    .otherwise(0))\n",
    "    .dropDuplicates(\"sessID\")\n",
    "    .drop(\"itemID\")\n",
    "    .drop(\"date\")\n",
    "    .drop(\"sDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When there is not a purchase, count of category types : \n",
      "+---------+-------+\n",
      "|specOffer|  count|\n",
      "+---------+-------+\n",
      "|  special|3054666|\n",
      "|   normal|1548626|\n",
      "|    brand|  18074|\n",
      "|  missing|4118667|\n",
      "+---------+-------+\n",
      "\n",
      "When there is a purchase, count of category types : \n",
      "+---------+------+\n",
      "|specOffer| count|\n",
      "+---------+------+\n",
      "|  special|168369|\n",
      "|   normal| 89143|\n",
      "|    brand|   730|\n",
      "|  missing|251454|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"When there is not a purchase, count of category types : \")\n",
    "messyPreDF.filter(col(\"purchased\")===0).groupBy(\"specOffer\").count().show()\n",
    "println(\"When there is a purchase, count of category types : \")\n",
    "messyPreDF.filter(col(\"purchased\")===1).groupBy(\"specOffer\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When there is not a purchase, count of category types : \n",
      "+-------+-------+\n",
      "|weekend|  count|\n",
      "+-------+-------+\n",
      "|      1|2815085|\n",
      "|      0|5924948|\n",
      "+-------+-------+\n",
      "\n",
      "When there is a purchase, count of category types : \n",
      "+-------+------+\n",
      "|weekend| count|\n",
      "+-------+------+\n",
      "|      1|194669|\n",
      "|      0|315027|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"When there is not a purchase, count of category types : \")\n",
    "messyPreDF.filter(col(\"purchased\")===0).groupBy(\"weekend\").count().show()\n",
    "println(\"When there is a purchase, count of category types : \")\n",
    "messyPreDF.filter(col(\"purchased\")===1).groupBy(\"weekend\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When there is not a purchase, count of category types : \n",
      "+---+-------+\n",
      "|day|  count|\n",
      "+---+-------+\n",
      "|Sun|1838984|\n",
      "|Mon|1782983|\n",
      "|Thu|1246785|\n",
      "|Sat| 976101|\n",
      "|Wed|1300885|\n",
      "|Tue| 623987|\n",
      "|Fri| 970308|\n",
      "+---+-------+\n",
      "\n",
      "When there is a purchase, count of category types : \n",
      "+---+------+\n",
      "|day| count|\n",
      "+---+------+\n",
      "|Sun|122462|\n",
      "|Mon| 99046|\n",
      "|Thu| 68629|\n",
      "|Sat| 72207|\n",
      "|Wed| 68336|\n",
      "|Fri| 58132|\n",
      "|Tue| 20884|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"When there is not a purchase, count of category types : \")\n",
    "messyPreDF.filter(col(\"purchased\")===0).groupBy(\"day\").count().show()\n",
    "println(\"When there is a purchase, count of category types : \")\n",
    "messyPreDF.filter(col(\"purchased\")===1).groupBy(\"day\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When there is not a purchase, count of category types : \n",
      "+---------+-------+\n",
      "|  hStatus|  count|\n",
      "+---------+-------+\n",
      "|afternoon|2953906|\n",
      "|  morning|2964755|\n",
      "|latenight| 439608|\n",
      "|  evening|2248916|\n",
      "| midnight| 132848|\n",
      "+---------+-------+\n",
      "\n",
      "When there is a purchase, count of category types : \n",
      "+---------+------+\n",
      "|  hStatus| count|\n",
      "+---------+------+\n",
      "|afternoon|190848|\n",
      "|  morning|178765|\n",
      "|latenight| 16403|\n",
      "|  evening|120226|\n",
      "| midnight|  3454|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"When there is not a purchase, count of category types : \")\n",
    "messyPreDF.filter(col(\"purchased\")===0).groupBy(\"hStatus\").count().show()\n",
    "println(\"When there is a purchase, count of category types : \")\n",
    "messyPreDF.filter(col(\"purchased\")===1).groupBy(\"hStatus\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cateListContains: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function2>,IntegerType,Some(List(StringType, StringType)))\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// For one-hot encoding,\"that is not Spark Way\" need some UDF approach even I don't want to use them a lot instead of built-ins.\n",
    "val cateListContains  =\n",
    "      udf((cateList: String, item: String) => if (cateList.contains(item)) 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "oneHotEncoderDF: (dfParam: org.apache.spark.sql.DataFrame, dfCol: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// What is this function do is actually folding left from Target column that belongs to DataFrame, collect unique values \n",
    "// check whether particular column does have the value that we add as a new column.\n",
    "def oneHotEncoderDF(dfParam: DataFrame,dfCol:String): DataFrame ={\n",
    "    val targetCols = dfParam.select(dfCol).distinct.collect.map(x=>x(0).toString)\n",
    "    targetCols.foldLeft(dfParam) {\n",
    "      case (df, item) =>\n",
    "        df.withColumn(item, cateListContains(col(dfCol), lit(item)))\n",
    "}.drop(dfCol)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colIterator: (df: org.apache.spark.sql.DataFrame, targetCols: List[String])org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Instead of imperative way, I prefer functional way to define, drill into. This is the functions one-hot encode columns,\n",
    "// drops the old categorical variable.\n",
    "def colIterator(df: DataFrame,targetCols: List[String]) : DataFrame=\n",
    "targetCols match{\n",
    "    case Nil => df\n",
    "    case(xs) => colIterator(oneHotEncoderDF(df,xs.head),xs.tail)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----+-------+--------------------+-------------+-----------+-----------+---------+---------+-------+---------+-------+--------+-------+------+-----+-------+\n",
      "|  sessID|day|hour|weekend|passedInSess(minute)|dayPopularity|clicksInDay|clicksInAll|purchased|afternoon|morning|latenight|evening|midnight|special|normal|brand|missing|\n",
      "+--------+---+----+-------+--------------------+-------------+-----------+-----------+---------+---------+-------+---------+-------+--------+-------+------+-----+-------+\n",
      "|10000108|Sun|  18|      1|  10.433333333333334|       393589|       1990|       8073|        0|        0|      0|        0|      1|       0|      1|     0|    0|      0|\n",
      "|10000172|Mon|   5|      0|   3.466666666666667|       338077|        533|       2642|        0|        0|      0|        1|      0|       0|      1|     0|    0|      0|\n",
      "|10000304|Sat|  11|      1|                0.15|        97536|         35|        881|        0|        0|      1|        0|      0|       0|      0|     1|    0|      0|\n",
      "|10000454|Mon|  12|      0|                 0.0|       338077|       7287|      37077|        0|        1|      0|        0|      0|       0|      1|     0|    0|      0|\n",
      "|10000472|Tue|   7|      0|                3.15|        55470|        110|       6953|        0|        0|      1|        0|      0|       0|      0|     0|    0|      1|\n",
      "|10000528|Sun|  16|      1|                 1.7|       393589|      11964|      37077|        0|        1|      0|        0|      0|       0|      1|     0|    0|      0|\n",
      "|10000591|Sat|  21|      1|                 0.0|        97536|          5|       6518|        0|        0|      0|        0|      1|       0|      0|     1|    0|      0|\n",
      "|10000723|Sun|   9|      1|  0.7666666666666667|       393589|         75|       2801|        0|        0|      1|        0|      0|       0|      0|     1|    0|      0|\n",
      "|10000761|Sat|   8|      1|  2.2333333333333334|        97536|          4|       1276|        0|        0|      1|        0|      0|       0|      0|     1|    0|      0|\n",
      "|10000989|Sun|  20|      1|                 0.0|       393589|        926|      14016|        0|        0|      0|        0|      1|       0|      1|     0|    0|      0|\n",
      "|10001331|Mon|  12|      0|  1.6833333333333333|       338077|       8076|      44614|        0|        1|      0|        0|      0|       0|      1|     0|    0|      0|\n",
      "|10001922|Thu|   8|      0|                 1.7|       203255|          7|         64|        0|        0|      1|        0|      0|       0|      0|     1|    0|      0|\n",
      "|10001989|Mon|  10|      0|   2.283333333333333|       338077|          2|        240|        0|        0|      1|        0|      0|       0|      0|     1|    0|      0|\n",
      "|10002011|Mon|   8|      0|                 0.0|       338077|       1964|      14169|        0|        0|      1|        0|      0|       0|      1|     0|    0|      0|\n",
      "|10002674|Fri|  12|      0|   38.68333333333333|       154637|         29|       7961|        0|        1|      0|        0|      0|       0|      1|     0|    0|      0|\n",
      "|10002811|Wed|  18|      0|  2.1666666666666665|        60312|        302|       8867|        0|        0|      0|        0|      1|       0|      1|     0|    0|      0|\n",
      "|10003366|Mon|  12|      0|                 4.5|       338077|       2622|      18763|        0|        1|      0|        0|      0|       0|      1|     0|    0|      0|\n",
      "|10004662|Fri|  13|      0|   4.683333333333334|       154637|        202|      20293|        0|        1|      0|        0|      0|       0|      1|     0|    0|      0|\n",
      "|10004759|Tue|   8|      0|                 0.0|        55470|         98|       2412|        0|        0|      1|        0|      0|       0|      1|     0|    0|      0|\n",
      "|10004786|Thu|   6|      0|  18.433333333333334|       203255|         36|       8866|        0|        0|      1|        0|      0|       0|      0|     1|    0|      0|\n",
      "+--------+---+----+-------+--------------------+-------------+-----------+-----------+---------+---------+-------+---------+-------+--------+-------+------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Given list of column names, my function can handle that this problem.\n",
    "colIterator(messyPreDF,List(\"hStatus\",\"specOffer\")).drop(\"type\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sessID: string (nullable = true)\n",
      " |-- weekend: integer (nullable = false)\n",
      " |-- passedInSess(minute): double (nullable = false)\n",
      " |-- dayPopularity: long (nullable = false)\n",
      " |-- clicksInDay: long (nullable = false)\n",
      " |-- clicksInAll: long (nullable = false)\n",
      " |-- purchased: integer (nullable = false)\n",
      " |-- Sun: integer (nullable = false)\n",
      " |-- Mon: integer (nullable = false)\n",
      " |-- Thu: integer (nullable = false)\n",
      " |-- Sat: integer (nullable = false)\n",
      " |-- Wed: integer (nullable = false)\n",
      " |-- Tue: integer (nullable = false)\n",
      " |-- Fri: integer (nullable = false)\n",
      " |-- afternoon: integer (nullable = false)\n",
      " |-- morning: integer (nullable = false)\n",
      " |-- latenight: integer (nullable = false)\n",
      " |-- evening: integer (nullable = false)\n",
      " |-- midnight: integer (nullable = false)\n",
      " |-- special: integer (nullable = false)\n",
      " |-- normal: integer (nullable = false)\n",
      " |-- brand: integer (nullable = false)\n",
      " |-- missing: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Now the schema is seems trainable with numerical values\n",
    "colIterator(messyPreDF,List(\"day\",\"hStatus\",\"specOffer\")).drop(\"type\",\"hour\").na.fill(0).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "47: error: not found: value messyPreDF\r",
     "output_type": "error",
     "traceback": [
      "<console>:47: error: not found: value messyPreDF\r",
      "       val finalDF = colIterator(messyPreDF,List(\"hStatus\",\"specOffer\")).drop(\"type\",\"hour\",\"day\").na.fill(0)\r",
      "                                 ^",
      ""
     ]
    }
   ],
   "source": [
    "// At the end of preprocessing techniques, baseline model is ready for training.\n",
    "val finalDF = colIterator(messyPreDF,List(\"hStatus\",\"specOffer\")).drop(\"type\",\"hour\",\"day\").na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featuresCol: Array[String] = Array(weekend, passedInSess(minute), dayPopularity, clicksInDay, clicksInAll, afternoon, morning, latenight, evening, midnight, special, normal, brand, missing)\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Index and label columns dropped to determine features column.\n",
    "val featuresCol  = finalDF.columns.filter(!Array(\"sessID\",\"purchased\").contains(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Phase\n",
    "### This process can be done by making checklist done as well as experimenting it. We don't have any prior knowledge about which techniques increase the accuracy, AUC, recall, precision ,f1 etc. We start to doing that by creating baseline model as simple as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_798704f44f3d\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This small code block transforms features columns to feature vector.\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(featuresCol)\n",
    "  .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembledDF: org.apache.spark.sql.DataFrame = [sessID: string, weekend: int ... 15 more fields]\r\n",
       "res6: assembledDF.type = [sessID: string, weekend: int ... 15 more fields]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// When it is transformed \n",
    "val assembledDF = assembler.transform(finalDF).withColumnRenamed(\"purchased\",\"label\").withColumn(\"label\",col(\"label\").cast(DoubleType))\n",
    "assembledDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [sessID: string, weekend: int ... 15 more fields]\r\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [sessID: string, weekend: int ... 15 more fields]\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training, test) = assembledDF.randomSplit(Array(0.8,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_6e716be86d71\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrmodel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_6e716be86d71, numClasses = 2, numFeatures = 14\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrmodel = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionslr: org.apache.spark.sql.DataFrame = [sessID: string, weekend: int ... 18 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionslr = lrmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluatorlr: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_bc0ac8e1c46e\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluatorlr = new BinaryClassificationEvaluator()\n",
    "                .setRawPredictionCol(\"rawPrediction\")\n",
    "                .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Double = 0.5\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorlr.evaluate(predictionslr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_ca62c939d298\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf = new RandomForestClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfmodel: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_ca62c939d298) with 20 trees\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfmodel = rf.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionsrf: org.apache.spark.sql.DataFrame = [sessID: string, weekend: int ... 18 more fields]\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionsrf = rfmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluatorrf: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_289cf702ebf9\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluatorrf = new BinaryClassificationEvaluator()\n",
    "                .setRawPredictionCol(\"rawPrediction\")\n",
    "                .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: String = areaUnderROC\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorrf.getMetricName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Double = 0.6885591158104136\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorrf.evaluate(predictionsrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: For This Baseline Model, I preferred one linear and one non-linear model to see which one is performing better. For areaUnderROC Random Forest made a good job. If next baseline models will not perform well as good as this RF Model, this baseline features and model will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis for Beliefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|hour| count|\n",
      "+----+------+\n",
      "|  19|650709|\n",
      "|  18|622187|\n",
      "|   9|551643|\n",
      "|  17|542025|\n",
      "|  10|533636|\n",
      "|  20|526799|\n",
      "|   8|519291|\n",
      "|  11|516324|\n",
      "|  16|491848|\n",
      "|  12|487862|\n",
      "|  15|487667|\n",
      "|  14|480026|\n",
      "|   7|468462|\n",
      "|  13|464469|\n",
      "|   6|375326|\n",
      "|  21|301338|\n",
      "|   5|249731|\n",
      "|  22|147858|\n",
      "|   4|118520|\n",
      "|  23| 68230|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messyPreDF.filter(col(\"purchased\")===0.0).groupBy(\"hour\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|hour|count|\n",
      "+----+-----+\n",
      "|  18|41203|\n",
      "|  19|37135|\n",
      "|  17|36936|\n",
      "|   9|33439|\n",
      "|   8|32689|\n",
      "|  11|32635|\n",
      "|  16|32537|\n",
      "|  10|32522|\n",
      "|  15|32228|\n",
      "|  14|30279|\n",
      "|  12|30149|\n",
      "|  13|28707|\n",
      "|   7|27830|\n",
      "|  20|24851|\n",
      "|   6|19650|\n",
      "|  21|12197|\n",
      "|   5|10259|\n",
      "|  22| 4867|\n",
      "|   4| 4182|\n",
      "|  23| 1839|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messyPreDF.filter(col(\"purchased\")===1.0).groupBy(\"hour\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|day|  count|\n",
      "+---+-------+\n",
      "|Sun|1839005|\n",
      "|Mon|1782959|\n",
      "|Wed|1300892|\n",
      "|Thu|1246775|\n",
      "|Sat| 976109|\n",
      "|Fri| 970288|\n",
      "|Tue| 624005|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messyPreDF.filter(col(\"purchased\")===0.0).groupBy(\"day\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|day| count|\n",
      "+---+------+\n",
      "|Sun|122463|\n",
      "|Mon| 99045|\n",
      "|Sat| 72205|\n",
      "|Thu| 68631|\n",
      "|Wed| 68332|\n",
      "|Fri| 58134|\n",
      "|Tue| 20886|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messyPreDF.filter(col(\"purchased\")===1.0).groupBy(\"day\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|  hStatus|  count|\n",
      "+---------+-------+\n",
      "|  morning|2964682|\n",
      "|afternoon|2953897|\n",
      "|  evening|2248891|\n",
      "|latenight| 439659|\n",
      "| midnight| 132904|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messyPreDF.filter(col(\"purchased\")===0.0).groupBy(\"hStatus\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|  hStatus| count|\n",
      "+---------+------+\n",
      "|afternoon|190836|\n",
      "|  morning|178765|\n",
      "|  evening|120253|\n",
      "|latenight| 16376|\n",
      "| midnight|  3466|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messyPreDF.filter(col(\"purchased\")===1.0).groupBy(\"hStatus\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---+----+---------+-------+---------+--------------------+-------------+-----------+-----------+---------+\n",
      "|  sessID|type|day|hour|  hStatus|weekend|specOffer|passedInSess(minute)|dayPopularity|clicksInDay|clicksInAll|purchased|\n",
      "+--------+----+---+----+---------+-------+---------+--------------------+-------------+-----------+-----------+---------+\n",
      "|10000108|   S|Sun|  18|  evening|      1|  special|  10.433333333333334|       393589|       1990|       8073|        0|\n",
      "|10000172|   S|Mon|   5|latenight|      0|  special|   3.466666666666667|       338077|       1297|       7274|        0|\n",
      "|10000304|   5|Sat|  11|  morning|      1|   normal|                0.15|        97536|         35|        881|        0|\n",
      "|10000454|   S|Mon|  12|afternoon|      0|  special|                 0.0|       338077|       7287|      37077|        0|\n",
      "|10000472|   0|Tue|   7|  morning|      0|  missing|                3.15|        55470|        110|       6953|        0|\n",
      "|10000528|   S|Sun|  16|afternoon|      1|  special|                 1.7|       393589|      11964|      37077|        0|\n",
      "|10000591|   1|Sat|  21|  evening|      1|   normal|                 0.0|        97536|          5|       6518|        0|\n",
      "|10000723|   3|Sun|   9|  morning|      1|   normal|  0.7666666666666667|       393589|         75|       2801|        0|\n",
      "|10000761|   2|Sat|   8|  morning|      1|   normal|  2.2333333333333334|        97536|          4|       1276|        0|\n",
      "|10000989|   S|Sun|  20|  evening|      1|  special|                 0.0|       393589|        926|      14016|        0|\n",
      "+--------+----+---+----+---------+-------+---------+--------------------+-------------+-----------+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messyPreDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From statistical analysis, we are going to try play with variables, variable values, such as extracting night variable from from two hour ranges(midnight,latenight) etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alphaOneDF: org.apache.spark.sql.DataFrame = [sessID: string, hStatus: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val alphaOneDF = messyPreDF.withColumn(\"night\",\n",
    "                        when(col(\"hStatus\")===\"midnight\"||col(\"hStatus\")===\"latenight\",1).otherwise(0))\n",
    "          .withColumn(\"weekendlk\"\n",
    "                      ,when(col(\"day\")===\"Sat\",1)\n",
    "                      .when(col(\"day\")===\"Sun\",1)\n",
    "                      .when(col(\"day\")===\"Mon\",1)\n",
    "                      .otherwise(0)).withColumnRenamed(\"passedInSess(minute)\",\"pInSess\")\n",
    "                      .withColumn(\"workOut\",when(col(\"hour\")>17 && col(\"hour\")<19,1).otherwise(0))\n",
    "                      .drop(\"type\",\"day\",\"hour\",\"weekend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alphaTwoDF: org.apache.spark.sql.DataFrame = [sessID: string, pInSess: double ... 16 more fields]\n"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val alphaTwoDF = colIterator(alphaOneDF,List(\"specOffer\",\"hStatus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updatedFeatureColOne: Array[String] = Array(pInSess, dayPopularity, clicksInDay, clicksInAll, night, weekendlk, workOut, special, normal, brand, missing, afternoon, morning, latenight, evening, midnight)\n"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val updatedFeatureColOne = alphaTwoDF.columns.filter(!Array(\"sessID\",\"purchased\").contains(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_dddf8cb9b832\n"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaler = new StandardScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"scaledFeatures\")\n",
    "  .setWithStd(true)\n",
    "  .setWithMean(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As well as feature transformations, scaling can help us a lot. So 'll try scaling approach for baselines one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-------------+-----------+-----------+-----+-----+---------+-------+-------+------+-----+-------+---------+-------+---------+-------+--------+--------------------+--------------------+\n",
      "|  sessID|           pInSess|dayPopularity|clicksInDay|clicksInAll|label|night|weekendlk|workOut|special|normal|brand|missing|afternoon|morning|latenight|evening|midnight|            features|      scaledFeatures|\n",
      "+--------+------------------+-------------+-----------+-----------+-----+-----+---------+-------+-------+------+-----+-------+---------+-------+---------+-------+--------+--------------------+--------------------+\n",
      "|10000108|10.433333333333334|       393589|       1990|       8073|    0|    0|        1|      1|      1|     0|    0|      0|        0|      0|        0|      1|       0|(16,[0,1,2,3,5,6,...|(16,[0,1,2,3,5,6,...|\n",
      "|10000172| 3.466666666666667|       338077|       1297|       7274|    0|    1|        1|      0|      1|     0|    0|      0|        0|      0|        1|      0|       0|(16,[0,1,2,3,4,5,...|(16,[0,1,2,3,4,5,...|\n",
      "|10000304|              0.15|        97536|         35|        881|    0|    0|        1|      0|      0|     1|    0|      0|        0|      1|        0|      0|       0|(16,[0,1,2,3,5,8,...|(16,[0,1,2,3,5,8,...|\n",
      "|10000454|               0.0|       338077|       7287|      37077|    0|    0|        1|      0|      1|     0|    0|      0|        1|      0|        0|      0|       0|(16,[1,2,3,5,7,11...|(16,[1,2,3,5,7,11...|\n",
      "|10000472|              3.15|        55470|        110|       6953|    0|    0|        0|      0|      0|     0|    0|      1|        0|      1|        0|      0|       0|(16,[0,1,2,3,10,1...|(16,[0,1,2,3,10,1...|\n",
      "|10000528|               1.7|       393589|      11964|      37077|    0|    0|        1|      0|      1|     0|    0|      0|        1|      0|        0|      0|       0|(16,[0,1,2,3,5,7,...|(16,[0,1,2,3,5,7,...|\n",
      "|10000591|               0.0|        97536|          5|       6518|    0|    0|        1|      0|      0|     1|    0|      0|        0|      0|        0|      1|       0|(16,[1,2,3,5,8,14...|(16,[1,2,3,5,8,14...|\n",
      "|10000723|0.7666666666666667|       393589|         75|       2801|    0|    0|        1|      0|      0|     1|    0|      0|        0|      1|        0|      0|       0|(16,[0,1,2,3,5,8,...|(16,[0,1,2,3,5,8,...|\n",
      "|10000761|2.2333333333333334|        97536|          4|       1276|    0|    0|        1|      0|      0|     1|    0|      0|        0|      1|        0|      0|       0|(16,[0,1,2,3,5,8,...|(16,[0,1,2,3,5,8,...|\n",
      "|10000989|               0.0|       393589|        926|      14016|    0|    0|        1|      0|      1|     0|    0|      0|        0|      0|        0|      1|       0|(16,[1,2,3,5,7,14...|(16,[1,2,3,5,7,14...|\n",
      "|10001331|1.6833333333333333|       338077|       8076|      44614|    0|    0|        1|      0|      1|     0|    0|      0|        1|      0|        0|      0|       0|(16,[0,1,2,3,5,7,...|(16,[0,1,2,3,5,7,...|\n",
      "|10001922|               1.7|       203255|          7|         64|    0|    0|        0|      0|      0|     1|    0|      0|        0|      1|        0|      0|       0|(16,[0,1,2,3,8,12...|(16,[0,1,2,3,8,12...|\n",
      "|10001989| 2.283333333333333|       338077|          2|        240|    0|    0|        1|      0|      0|     1|    0|      0|        0|      1|        0|      0|       0|(16,[0,1,2,3,5,8,...|(16,[0,1,2,3,5,8,...|\n",
      "|10002011|               0.0|       338077|       1964|      14169|    0|    0|        1|      0|      1|     0|    0|      0|        0|      1|        0|      0|       0|(16,[1,2,3,5,7,12...|(16,[1,2,3,5,7,12...|\n",
      "|10002674| 38.68333333333333|       154637|        345|       4778|    0|    0|        0|      0|      1|     0|    0|      0|        1|      0|        0|      0|       0|(16,[0,1,2,3,7,11...|(16,[0,1,2,3,7,11...|\n",
      "|10002811|2.1666666666666665|        60312|        302|       8867|    0|    0|        0|      1|      1|     0|    0|      0|        0|      0|        0|      1|       0|(16,[0,1,2,3,6,7,...|(16,[0,1,2,3,6,7,...|\n",
      "|10003366|               4.5|       338077|       1631|       8073|    0|    0|        1|      0|      1|     0|    0|      0|        1|      0|        0|      0|       0|(16,[0,1,2,3,5,7,...|(16,[0,1,2,3,5,7,...|\n",
      "|10004662| 4.683333333333334|       154637|        202|      20293|    0|    0|        0|      0|      1|     0|    0|      0|        1|      0|        0|      0|       0|(16,[0,1,2,3,7,11...|(16,[0,1,2,3,7,11...|\n",
      "|10004759|               0.0|        55470|         98|       2412|    0|    0|        0|      0|      1|     0|    0|      0|        0|      1|        0|      0|       0|(16,[1,2,3,7,12],...|(16,[1,2,3,7,12],...|\n",
      "|10004786|18.433333333333334|       203255|         36|       8866|    0|    0|        0|      0|      0|     1|    0|      0|        0|      1|        0|      0|       0|(16,[0,1,2,3,8,12...|(16,[0,1,2,3,8,12...|\n",
      "+--------+------------------+-------------+-----------+-----------+-----+-----+---------+-------+-------+------+-----+-------+---------+-------+---------+-------+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "scalerModelOne: org.apache.spark.ml.feature.StandardScalerModel = stdScal_dddf8cb9b832\r\n",
       "scaledDataOne: org.apache.spark.sql.DataFrame = [sessID: string, pInSess: double ... 18 more fields]\n"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scalerModelOne = scaler.fit(assembledDFOne)\n",
    "\n",
    "// Normalize each feature to have unit standard deviation.\n",
    "val scaledDataOne = scalerModelOne.transform(assembledDFOne).withColumnRenamed(\"purchased\",\"label\")\n",
    "scaledDataOne.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [sessID: string, hour: int ... 23 more fields]\r\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [sessID: string, hour: int ... 23 more fields]\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training,test) = scaledDataOne.randomSplit(Array(0.8,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_5c380becfa90\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf = new RandomForestClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfModel: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_5c380becfa90) with 20 trees\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfModel = rf.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfPreds: org.apache.spark.sql.DataFrame = [sessID: string, hour: int ... 26 more fields]\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfPreds = rfModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluatorRf: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_edd1bf2d6f31\n"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluatorRf = new BinaryClassificationEvaluator()\n",
    "                .setRawPredictionCol(\"rawPrediction\")\n",
    "                .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res43: Double = 0.5\n"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorRf.evaluate(rfPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_1f7d3bca7c3c, numClasses = 2, numFeatures = 17\n"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrPreds: org.apache.spark.sql.DataFrame = [sessID: string, weekend: int ... 22 more fields]\n"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrPreds = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res73: Double = 0.5\n"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorlr.evaluate(lrPreds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This model performs worse than previous model. So we will not use this combination anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res188: Array[String] = Array(sessID, pInSess, dayPopularity, clicksInDay, clicksInAll, purchased, night, weekendlk, workOut, special, normal, brand, missing, afternoon, morning, latenight, evening, midnight)\n"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphaTwoDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking Correlations can help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res193: Double = 0.6145526630737359\n"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphaTwoDF.stat.corr(\"clicksInDay\",\"clicksInAll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res200: Double = 0.8705543522756433\n"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphaTwoDF.stat.corr(\"night\",\"latenight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alphaThreeDF: org.apache.spark.sql.DataFrame = [sessID: string, pInSess: double ... 13 more fields]\n"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val alphaThreeDF = alphaTwoDF.drop(\"midnight\",\"latenight\",\"clicksInDay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "|day|hour| count|\n",
      "+---+----+------+\n",
      "|Sun|  18|154941|\n",
      "|Sun|  19|150458|\n",
      "|Sun|   9|145022|\n",
      "|Sun|  17|139771|\n",
      "|Sun|  10|132639|\n",
      "|Wed|  19|131506|\n",
      "|Mon|  18|128533|\n",
      "|Sun|  16|125256|\n",
      "|Mon|  19|125097|\n",
      "|Sun|  11|122582|\n",
      "|Sun|  15|122028|\n",
      "|Sun|   8|120213|\n",
      "|Sun|  20|119525|\n",
      "|Mon|  17|119466|\n",
      "|Mon|   9|118981|\n",
      "|Mon|   8|118174|\n",
      "|Mon|  10|116924|\n",
      "|Mon|   7|114440|\n",
      "|Wed|  18|114040|\n",
      "|Sun|  14|113439|\n",
      "+---+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messyPreDF.groupBy(\"day\",\"hour\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hourRangeABTester: (startPt: Int, endPt: Int)Long\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Functions take start and end range, and calculates how many sales made between ranges\n",
    "def hourRangeABTester(startPt: Int, endPt:Int) = {\n",
    "    messyPreDF.withColumn(\"importantHalf\", when(col(\"hour\")>startPt && col(\"hour\")<=endPt,1).otherwise(0)).filter(col(\"purchased\")===col(\"importantHalf\"))count()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Array[Long] = Array(3796731, 4738258, 5173985, 5623761, 6434927, 5396014)\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Maximize day sales by obtaining best range for buying particular to this dataset.\n",
    "Array((10,20),(12,20),(13,20),(14,20),(12,17),(11,18)).map(x=>hourRangeABTester(x._1,x._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12PM-17PM seems good range to be predictor\n",
    "##### res19: Array[Long] = Array(3796731, 4738258, 5173985, 5623761, 6434927, 5396014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alphaFourDF: org.apache.spark.sql.DataFrame = [sessID: string, hour: int ... 15 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val alphaFourDF= colIterator(messyPreDF.withColumn(\"importantHalf\",\n",
    "                                  when(col(\"hour\")>12&& col(\"hour\")<=17,1).otherwise(0))\n",
    "            ,List(\"hStatus\",\"specoffer\"))\n",
    "            .withColumnRenamed(\"passedInSess(minute)\",\"pInSess\")\n",
    "            .withColumnRenamed(\"purchased\",\"label\")\n",
    "            .drop(\"clicksInDay\",\"type\",\"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updatedFeatureColFourAlpha: Array[String] = Array(hour, weekend, pInSess, dayPopularity, clicksInAll, importantHalf, afternoon, morning, latenight, evening, midnight, special, normal, brand, missing)\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val updatedFeatureColFourAlpha = alphaFourDF.columns.filter(!Array(\"sessID\",\"label\").contains(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sessID: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekend: integer (nullable = false)\n",
      " |-- pInSess: double (nullable = true)\n",
      " |-- dayPopularity: long (nullable = false)\n",
      " |-- clicksInAll: long (nullable = false)\n",
      " |-- label: integer (nullable = false)\n",
      " |-- importantHalf: integer (nullable = false)\n",
      " |-- afternoon: integer (nullable = false)\n",
      " |-- morning: integer (nullable = false)\n",
      " |-- latenight: integer (nullable = false)\n",
      " |-- evening: integer (nullable = false)\n",
      " |-- midnight: integer (nullable = false)\n",
      " |-- special: integer (nullable = false)\n",
      " |-- normal: integer (nullable = false)\n",
      " |-- brand: integer (nullable = false)\n",
      " |-- missing: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphaFourDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_d01a57a96dbc\r\n",
       "assembledDFFour: org.apache.spark.sql.DataFrame = [sessID: string, hour: int ... 16 more fields]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(updatedFeatureColFourAlpha)\n",
    "  .setOutputCol(\"features\")\n",
    "val assembledDFFour = assembler.transform(alphaFourDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [sessID: string, hour: int ... 16 more fields]\r\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [sessID: string, hour: int ... 16 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training,test) = assembledDFFour.na.fill(0.0).randomSplit(Array(0.8,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_67562365115c\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf = new RandomForestClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfModel: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_57fe9d1bfe21) with 20 trees\r\n",
       "rfPreds: org.apache.spark.sql.DataFrame = [sessID: string, hour: int ... 19 more fields]\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfModel = rf.fit(training)\n",
    "val rfPreds = rfModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluatorrf: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_c84c98a138f2\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluatorrf = new BinaryClassificationEvaluator()\n",
    "                .setRawPredictionCol(\"rawPrediction\")\n",
    "                .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Double = 0.5\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorrf.evaluate(rfPreds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Models Compared. Accuracy scores around 0.95 but AUC score is terrible. We care about AUC score. So we will continue with first model and tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "44: error: not found: value assembledDF\r",
     "output_type": "error",
     "traceback": [
      "<console>:44: error: not found: value assembledDF\r",
      "       val Array(trainingFinal, testFinal) = assembledDF.randomSplit(Array(0.8,0.2))\r",
      "                                             ^",
      ""
     ]
    }
   ],
   "source": [
    "val Array(trainingFinal, testFinal) = assembledDF.randomSplit(Array(0.8,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_4bb95aac4948\n"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf = new RandomForestClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfModelFinal: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_69a96f7ab686) with 20 trees\r\n",
       "rfPredFinal: org.apache.spark.sql.DataFrame = [sessID: string, weekend: int ... 18 more fields]\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfModelFinal = rf.fit(trainingFinal)\n",
    "val rfPredFinal = rfModelFinal.transform(testFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluatorrf: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_dec0f8244e1a\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluatorrf = new BinaryClassificationEvaluator()\n",
    "                .setRawPredictionCol(\"rawPrediction\")\n",
    "                .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: Double = 0.6809865725642923\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorrf.evaluate(rfPredFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stdtScalerFinal: org.apache.spark.ml.feature.StandardScaler = stdScal_42f5494f4046\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stdtScalerFinal = new StandardScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"scaledFeatures\")\n",
    "  .setWithStd(true)\n",
    "  .setWithMean(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfFinalScaled: org.apache.spark.ml.classification.RandomForestClassifier = rfc_dc38a9aaf699\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfFinalScaled = new RandomForestClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaledFinalDF: org.apache.spark.sql.DataFrame = [sessID: string, weekend: int ... 16 more fields]\n"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaledFinalDF = stdtScalerFinal.fit(assembledDF).transform(assembledDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Array[String] = Array(sessID, weekend, passedInSess(minute), dayPopularity, clicksInDay, clicksInAll, label, afternoon, morning, latenight, evening, midnight, special, normal, brand, missing, features, scaledFeatures)\n"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledFinalDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "traningFinalScaled: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [sessID: string, weekend: int ... 16 more fields]\r\n",
       "testFinalScaled: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [sessID: string, weekend: int ... 16 more fields]\n"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(traningFinalScaled, testFinalScaled) = scaledFinalDF.randomSplit(Array(0.8,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfModelScaledFinal: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_dc38a9aaf699) with 20 trees\n"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfModelScaledFinal = rfFinalScaled.fit(traningFinalScaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfPredScaledFinal: org.apache.spark.sql.DataFrame = [sessID: string, weekend: int ... 19 more fields]\n"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfPredScaledFinal = rfModelScaledFinal.transform(testFinalScaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: Double = 0.6823373771250568\n"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorrf.evaluate(rfPredScaledFinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can't be sure about scaling is not helping us, maybe different part of data can help us when we scale, we are going to use 5-CV to research deeply this issue. But for now scaling helped us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updatedCombiner: org.apache.spark.sql.DataFrame => org.apache.spark.sql.DataFrame = <function1>\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val updatedCombiner = dateInfExtractor _ andThen \n",
    "                          weekTypeExtractor _ andThen \n",
    "                          specOfferExtractor _ andThen \n",
    "                          sessAverageItemPriceViewed _ andThen\n",
    "                          timeSpentEachSession _ andThen\n",
    "                          dayPopularityExtractor _ andThen\n",
    "                          itemDayPopularityExtractor _ andThen\n",
    "                          itemGeneralPopularityExtractor _ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updatedCombinerDF: org.apache.spark.sql.DataFrame = [sessID: string, date: string ... 16 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val updatedCombinerDF = updatedCombiner(cStreamDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: updatedCombinerDF.type = [sessID: string, date: string ... 16 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updatedCombinerDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "messyBetaDF: org.apache.spark.sql.DataFrame = [sessID: string, type: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val messyBetaDF = updatedCombinerDF\n",
    "    .join(cStreamBasketDF.groupBy(\"sessID\").count(),Seq(\"sessID\"),\"left\")\n",
    "    .withColumnRenamed(\"count\",\"purchased\")\n",
    "    .withColumn(\"purchased\"\n",
    "    ,when(col(\"purchased\")>0,1)\n",
    "    .otherwise(0))\n",
    "    .dropDuplicates(\"sessID\")\n",
    "    .drop(\"itemID\")\n",
    "    .drop(\"date\")\n",
    "    .drop(\"sDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalUpdatedAlphaDF: org.apache.spark.sql.DataFrame = [sessID: string, type: string ... 16 more fields]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val finalUpdatedAlphaDF = messyBetaDF\n",
    "    .withColumn(\"clicksCombination\",col(\"clicksInDay\")/col(\"clicksInAll\"))\n",
    "    .withColumn(\"importantHalf\",\n",
    "               when(col(\"hour\")>12&& col(\"hour\")<=17,1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: finalUpdatedAlphaDF.type = [sessID: string, type: string ... 16 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalUpdatedAlphaDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sessID: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- hStatus: string (nullable = false)\n",
      " |-- weekend: integer (nullable = false)\n",
      " |-- specOffer: string (nullable = false)\n",
      " |-- sum(price): double (nullable = true)\n",
      " |-- nOfPVis: long (nullable = false)\n",
      " |-- avg(price): double (nullable = true)\n",
      " |-- stddev_samp(price): double (nullable = true)\n",
      " |-- passedInSess(minute): double (nullable = true)\n",
      " |-- dayPopularity: long (nullable = false)\n",
      " |-- clicksInDay: long (nullable = false)\n",
      " |-- clicksInAll: long (nullable = false)\n",
      " |-- purchased: integer (nullable = false)\n",
      " |-- clicksCombination: double (nullable = true)\n",
      " |-- importantHalf: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalUpdatedAlphaDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Maybe first three page can be important about prection power. But also it can be main page which means everyone can see.\n",
    "finalUpdatedAlphaDF.filter(col(\"purchased\")===1)\n",
    "    .groupBy(\"type\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalFeatAddedDF: org.apache.spark.sql.DataFrame = [sessID: string, type: string ... 17 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val finalFeatAddedDF = finalUpdatedAlphaDF.withColumn(\"mostlyVisitedPage\"\n",
    "                               ,when(col(\"type\")===\"1\"||col(\"type\")===\"2\"|| col(\"type\")===\"3\",1)\n",
    "                                .otherwise(0))\n",
    "                        .withColumnRenamed(\"purchased\",\"label\")\n",
    "                        .withColumn(\"label\",col(\"label\").cast(DoubleType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalAltDF: org.apache.spark.sql.DataFrame = [sessID: string, hStatus: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val finalAltDF = messyBetaDF\n",
    ".drop(\"hour\",\"type\",\"day\")\n",
    ".withColumnRenamed(\"purchased\",\"label\")\n",
    ".withColumn(\"label\",col(\"label\").cast(DoubleType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalAlphaDF: org.apache.spark.sql.DataFrame = [sessID: string, weekend: int ... 18 more fields]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val finalAlphaDF = colIterator(finalAltDF,List(\"specOffer\",\"hStatus\")).na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: finalAlphaDF.type = [sessID: string, weekend: int ... 18 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalAlphaDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sessID: string (nullable = true)\n",
      " |-- weekend: integer (nullable = false)\n",
      " |-- sum(price): double (nullable = false)\n",
      " |-- nOfPVis: long (nullable = false)\n",
      " |-- avg(price): double (nullable = false)\n",
      " |-- stddev_samp(price): double (nullable = false)\n",
      " |-- passedInSess(minute): double (nullable = false)\n",
      " |-- dayPopularity: long (nullable = false)\n",
      " |-- clicksInDay: long (nullable = false)\n",
      " |-- clicksInAll: long (nullable = false)\n",
      " |-- label: double (nullable = false)\n",
      " |-- special: integer (nullable = false)\n",
      " |-- normal: integer (nullable = false)\n",
      " |-- brand: integer (nullable = false)\n",
      " |-- missing: integer (nullable = false)\n",
      " |-- afternoon: integer (nullable = false)\n",
      " |-- morning: integer (nullable = false)\n",
      " |-- latenight: integer (nullable = false)\n",
      " |-- evening: integer (nullable = false)\n",
      " |-- midnight: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalAlphaDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featuresColAlpha: Array[String] = Array(weekend, sum(price), nOfPVis, avg(price), stddev_samp(price), passedInSess(minute), dayPopularity, clicksInDay, clicksInAll, special, normal, brand, missing, afternoon, morning, latenight, evening, midnight)\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Index and label columns dropped to determine features column.\n",
    "val featuresColAlpha  = finalAlphaDF.columns.filter(!Array(\"sessID\",\"label\").contains(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[String] = Array(weekend, sum(price), nOfPVis, avg(price), stddev_samp(price), passedInSess(minute), dayPopularity, clicksInDay, clicksInAll, special, normal, brand, missing, afternoon, morning, latenight, evening, midnight)\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresColAlpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assemblerAlpha: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_959b1e9884eb\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This small code block transforms features columns to feature vector.\n",
    "val assemblerAlpha = new VectorAssembler()\n",
    "  .setInputCols(featuresColAlpha)\n",
    "  .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembledAlphaDF: org.apache.spark.sql.DataFrame = [sessID: string, weekend: int ... 19 more fields]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembledAlphaDF = assemblerAlpha.transform(finalAlphaDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scalerFinalMaxAbsAlpha: org.apache.spark.ml.feature.MaxAbsScaler = maxAbsScal_7e10cdae4725\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scalerFinalMaxAbsAlpha = new MaxAbsScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"maxAbsScaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scalerFinalMinAlpha: org.apache.spark.ml.feature.MinMaxScaler = minMaxScal_a3ed58b42f22\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scalerFinalMinAlpha = new MinMaxScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"minMaxScaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scalerFinalStandartAlpha: org.apache.spark.ml.feature.StandardScaler = stdScal_34a2fd920ae4\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scalerFinalStandartAlpha = new StandardScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"standardScaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaledFinalAlphaDF: org.apache.spark.sql.DataFrame = [sessID: string, weekend: int ... 20 more fields]\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaledFinalAlphaDF = scalerFinalMaxAbsAlpha.fit(assembledAlphaDF)\n",
    "                    .transform(assembledAlphaDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingFinalAlpha: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [sessID: string, weekend: int ... 20 more fields]\r\n",
       "testFinalAlpha: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [sessID: string, weekend: int ... 20 more fields]\r\n",
       "res6: testFinalAlpha.type = [sessID: string, weekend: int ... 20 more fields]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainingFinalAlpha,testFinalAlpha) = scaledFinalAlphaDF.randomSplit(Array(0.8,0.2))\n",
    "trainingFinalAlpha.cache()\n",
    "testFinalAlpha.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfFinalAlphaScaled: org.apache.spark.ml.classification.RandomForestClassifier = rfc_d1a7b1a84798\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfFinalAlphaScaled = new RandomForestClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"maxAbsScaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfAlphaFinalModel: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_d1a7b1a84798) with 20 trees\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfAlphaFinalModel = rfFinalAlphaScaled.fit(trainingFinalAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfPredAlphaFinal: org.apache.spark.sql.DataFrame = [sessID: string, weekend: int ... 23 more fields]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfPredAlphaFinal = rfAlphaFinalModel.transform(testFinalAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluatorAlphaFinal: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_b0e27bb71aa7\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluatorAlphaFinal = new BinaryClassificationEvaluator()\n",
    "                .setRawPredictionCol(\"rawPrediction\")\n",
    "                .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Double = 0.7133986723907214\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// AUC SCORE\n",
    "evaluatorAlphaFinal.evaluate(rfPredAlphaFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "truePreds: Long = 1746874\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val truePreds = rfPredAlphaFinal\n",
    ".withColumn(\"acc\",when(col(\"prediction\")-col(\"label\")===0,1).otherwise(0)).agg(sum(\"acc\")).collect().toList(0)(0).asInstanceOf[Long]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "totalCount: Long = 1848905\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val totalCount = rfPredAlphaFinal.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels are %94 correctly predicted!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accScore: Long = 94\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accScore = truePreds*100/totalCount\n",
    "println(s\"Labels are %$accScore correctly predicted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Score =%94 , AUC Score = %7133\n",
    "### Since data is big and best scores captures based on Random Forest, Cannot use Cross Validation because it excepts Java Heap Error both ide and notebook. \n",
    "\n",
    "## I hope it helps !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updatedCombinerWindowed: org.apache.spark.sql.DataFrame => org.apache.spark.sql.DataFrame = <function1>\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Adding Average Time Between Sessions doesn't help to our model in this case.Just increased to non-linearity.\n",
    "val updatedCombinerWindowed = \n",
    "                          dateInfExtractor _ andThen \n",
    "                          weekTypeExtractor _ andThen \n",
    "                          specOfferExtractor _ andThen \n",
    "                          sessAverageItemPriceViewed _ andThen\n",
    "                          sessAverageTimeBSessions _ andThen\n",
    "                          timeSpentEachSession _ andThen\n",
    "                          dayPopularityExtractor _ andThen\n",
    "                          itemDayPopularityExtractor _ andThen\n",
    "                          itemGeneralPopularityExtractor _ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
